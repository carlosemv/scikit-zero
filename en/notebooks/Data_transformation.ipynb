{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_transformation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bael_qb82WFW"},"source":["# Data transformation"]},{"cell_type":"markdown","metadata":{"id":"efNVWanU2XGP"},"source":["Machine learning (ML) algorithms may be sensitive to data representation, scale, and/or distribution. In this notebook, we'll discuss the main techniques provided by scikit-learn for this kind of preprocessing. To see how they work in practice, let's use a dataset for house price prediction provided by Kaggle. To download it, follow the [first stage of this tutorial](https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0), which shows how to download access credentials for Kaggle (`kaggle.json`). Once you have downloaded the credentials, use the side menu to upload the file to Colab, and run the cells below:"]},{"cell_type":"code","metadata":{"id":"XdjBR_R07xtw","executionInfo":{"status":"ok","timestamp":1601404662563,"user_tz":180,"elapsed":890,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["import pandas as pd\n","import seaborn as sns\n","sns.set()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3xNvvtE2agy","executionInfo":{"status":"ok","timestamp":1601404671911,"user_tz":180,"elapsed":1028,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["!mkdir /root/.kaggle\n","!cp /content/kaggle.json /root/.kaggle/kaggle.json\n","!chmod 600 /root/.kaggle/kaggle.json"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGh1aG5N7uCm"},"source":["!kaggle datasets download -d prevek18/ames-housing-dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPutfFRK7vrn"},"source":["ames_housing = pd.read_csv(\"ames-housing-dataset.zip\")\n","ames_housing.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yj0IbBMTAfV4"},"source":["> Should any of the cells above fail, contact the maintainers of scikit-zero ;)\n","\n","Peeking the first rows we can see that this is a very large dataset w.r.t. number of features. Let's check how many samples it has:"]},{"cell_type":"code","metadata":{"id":"dnzpbsi57wjI"},"source":["ames_housing.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9dR0X9DAx3n"},"source":["> Kinda like iris, right? ü§£\n","\n","Let's start isolating the feature we would like to predict from the input features:"]},{"cell_type":"code","metadata":{"id":"iSGrpTHVWFDo","executionInfo":{"status":"ok","timestamp":1601404965354,"user_tz":180,"elapsed":814,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["X = ames_housing.drop([\"SalePrice\"], axis=1)\n","y = ames_housing[\"SalePrice\"]"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuTI0dFvWOc1"},"source":["> Note that the feature we want to predict is real-valued, so we have a **regression** problem.\n","\n","To make our analysis a bit more practical for the purposes of this notebook, we'll **filter out** features according to some pre-defined criteria:\n","* features that are clearly unrelated to the target feature;\n","* features that present more than 90% of missing values;\n","* features with low variance:\n","    * Categorical features: the most ocurring value is in more than 90% of the samples.\n","    * Numerical features: the 90% quantile matches the median.\n","    \n","> Note that this filtering is specific to this notebook and could likely represent a premature decision in the machine learning modeling process. We do it here to make the dataset a bit simpler. For this reason also, we pre-identified these features rather than computing them in the notebook."]},{"cell_type":"code","metadata":{"id":"BfD1VW3wCtrd","executionInfo":{"status":"ok","timestamp":1601405482749,"user_tz":180,"elapsed":980,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["unrelated = [\"Order\", \"PID\", \"MS SubClass\"]\n","many_missing = [\"Pool QC\", \"Misc Feature\", \"Alley\"]\n","low_variance_nominal = ['Paved Drive', 'Electrical', 'Functional', 'Central Air', 'Land Slope', 'Heating', 'Roof Matl', 'Condition 2']\n","low_variance_ordinal = ['Garage Cond', 'Street', 'Utilities']\n","low_variance_numerical = [\"Low Qual Fin SF\", \"Bsmt Half Bath\", \"Full Bath\", \"Kitchen AbvGr\",\n","                          \"Fireplaces\", \"3Ssn Porch\", \"Screen Porch\", \"Pool Area\", \"Misc Val\"]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"0etFjJ8JMGY8","executionInfo":{"status":"ok","timestamp":1601405489868,"user_tz":180,"elapsed":733,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["X = X.drop(unrelated + many_missing +\n","           low_variance_nominal + low_variance_ordinal,\n","           axis=1)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6aCn6Dvrj20"},"source":["> Even with this filtering, this dataset is still quite complex. In real life, we would never go for modelling before having conducted an exploratory data analysis. Here, we are making things a bit simplistic since we're skipping that, but bear with me :)"]},{"cell_type":"code","metadata":{"id":"L8H7pv4uIDYq"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeJrUwGeVIyR"},"source":["## Encoding ordinal features"]},{"cell_type":"markdown","metadata":{"id":"obynYwv7Ol6b"},"source":["To properly prepare the data for modelling, the first thing we need to do is isolate feature types. This is very important, since different types of feature demand different preparation techniques. From a quick look at the data, I have isolated the following three feature subsets:\n","\n","> If you don't understand the difference between feature types, check [pandas-zero](https://github.com/leobezerra/pandas-zero). \n","\n","> If you do, notice that a few ordinal features are being listed as nominal. The reason is that we do not have a rich data dictionary available, and for those features it was not straightforward to understand the ranking implied by the values."]},{"cell_type":"code","metadata":{"id":"V1dUqCMtz-3R","executionInfo":{"status":"ok","timestamp":1601405558238,"user_tz":180,"elapsed":804,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["nominal = [\"MS Zoning\", \"Lot Shape\", \"Land Contour\",  \"Lot Config\", \n","           \"Neighborhood\", \"Condition 1\", \"Bldg Type\", \"House Style\", \n","           \"Roof Style\", \"Exterior 1st\", \"Exterior 2nd\", \"Mas Vnr Type\", \n","           \"Foundation\", \"BsmtFin Type 1\", \"BsmtFin Type 2\", \"Garage Type\", \n","           \"Fence\", \"Sale Type\", \"Sale Condition\", \"Bsmt Exposure\", \"Garage Finish\"]\n","\n","ordinal = [\"Exter Qual\", \"Exter Cond\", \"Bsmt Qual\", \"Bsmt Cond\",  \"Heating QC\",\n","           \"Kitchen Qual\", \"Fireplace Qu\", \"Garage Qual\"]\n","\n","numerical = [\"Lot Frontage\", \"Lot Area\", \"Year Built\", \"Year Remod/Add\", \"Mas Vnr Area\",\n","             \"BsmtFin SF 1\", \"BsmtFin SF 2\", \"Bsmt Unf SF\", \"Total Bsmt SF\", \"1st Flr SF\",\n","             \"2nd Flr SF\", \"Low Qual Fin SF\", \"Gr Liv Area\", \"Bsmt Full Bath\", \"Bsmt Half Bath\", \n","             \"Full Bath\", \"Half Bath\", \"Bedroom AbvGr\", \"Kitchen AbvGr\", \"TotRms AbvGrd\",\n","             \"Fireplaces\", \"Garage Yr Blt\", \"Garage Cars\", \"Garage Area\", \"Wood Deck SF\",\n","             \"Open Porch SF\", \"Enclosed Porch\", \"3Ssn Porch\", \"Screen Porch\", \"Pool Area\",\n","             \"Misc Val\", \"Mo Sold\", \"Yr Sold\", \"Overall Cond\", \"Overall Qual\"]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8p_fJOBATPga"},"source":["Let's temporarily isolate the ordinal features to see how we can prepare them:"]},{"cell_type":"code","metadata":{"id":"93UpUcLEY962"},"source":["X_ordinal = X[ordinal]\n","X_ordinal.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwJOgkDIP2Qn"},"source":["Checking these features a bit we see that they follow a ranking pattern:\n","0. `TA`: to be assessed\n","1. `Po`: poor\n","2. `Fa`: fair\n","3. `Gd`: good\n","4. `Ex`: excellent\n","\n","The encoding we apply to ordinal features is converting each ranking category into an integer, so that higher ranks are represented by larger values. \n","\n","> `TA` is equivalent to a missing value, and we'll rank it 0. Again, this could be premature, but it's a reasonable choice here.\n","\n","Encoding ordinal features in scikit-learn can be done with the `OrdinalEncoder` preprocessor from the `preprocessing` module:"]},{"cell_type":"code","metadata":{"id":"S6lFNxrm193F","executionInfo":{"status":"ok","timestamp":1601406048041,"user_tz":180,"elapsed":1313,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["from sklearn.preprocessing import OrdinalEncoder"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJ4oD8WHR3Qg"},"source":["`OrdinalEncoder` takes as input a list of lists: \n","- one list per feature\n","- each list should indicate the ranking of the categories\n","\n","For our example, the same list can be used to indicate the ranking of the categories:"]},{"cell_type":"code","metadata":{"id":"LGEcUkrx7CqR","executionInfo":{"status":"ok","timestamp":1601406197682,"user_tz":180,"elapsed":1246,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["conditions = [\"TA\", \"Po\", \"Fa\", \"Gd\", \"Ex\"]"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vY6IIEAKQ82S"},"source":["One limitation of `OrdinalEncoder` is that we cannot indicate that the same list will be used for multiple features. To replicate the list as many times as the number of features we have, we're gonna use the list comprehension notation we've discussed back in pandas-zero:\n","\n","> the `for i in range(n_features)` means we will have `n_features` repetitions of `conditions`"]},{"cell_type":"code","metadata":{"id":"FZOh9OJh9xJ6","executionInfo":{"status":"ok","timestamp":1601406428951,"user_tz":180,"elapsed":1189,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["n_features = X_ordinal.shape[1]\n","condition_lists = [conditions for i in range(n_features)]"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzW1TN55TB5n"},"source":["We can now create our encoder for ordinal features:"]},{"cell_type":"code","metadata":{"id":"9K9L5ARERys5","executionInfo":{"status":"ok","timestamp":1601406678554,"user_tz":180,"elapsed":836,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["condition_encoder = OrdinalEncoder(categories=condition_lists)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9AWrF2BTHui"},"source":["Another limitation of `OrdinalEncoder` is that it cannot be applied to data with missing values. For this reason, we're gonna use a pipeline where we first impute `'TA'` to the missing values, and then apply the encoding:"]},{"cell_type":"code","metadata":{"id":"580Pi7WX8kMC","executionInfo":{"status":"ok","timestamp":1601406705112,"user_tz":180,"elapsed":1869,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["from sklearn.impute import SimpleImputer\n","condition_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"TA\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"_34xVwDs8Stf","executionInfo":{"status":"ok","timestamp":1601406716606,"user_tz":180,"elapsed":1177,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["from sklearn.pipeline import make_pipeline\n","ordinal_pipe = make_pipeline(condition_imputer, condition_encoder)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x26j-b8jTZd0"},"source":["When we use a pipeline solely for data preparation, we use `fit()` and `transform()`, rather than `fit()` and `predict()`. In this case, we'll make it simpler and use `fit_transform()` directly:\n","\n","> Note that we wrap the output of the pipeline in a `DataFrame` object and preserve the column values."]},{"cell_type":"code","metadata":{"id":"cLtgNYA675j0"},"source":["X_ordinal = pd.DataFrame(ordinal_pipe.fit_transform(X_ordinal), columns=X_ordinal.columns)\n","X_ordinal.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"divVzWj4Tx0V"},"source":["Now our ordinal features have been encoded properly, and we can move on to nominal features."]},{"cell_type":"markdown","metadata":{"id":"CXljQ9pxUvhP"},"source":["## Encoding nominal features"]},{"cell_type":"markdown","metadata":{"id":"CEPebp2wU3nm"},"source":["Once again, let's start isolating our nominal features:"]},{"cell_type":"code","metadata":{"id":"plYwvAOsU3z_"},"source":["X_nominal = X[nominal]\n","X_nominal.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYMF_kPDj-JN"},"source":["X_nominal.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZsH3D1HXkz4"},"source":["The most common approach to nominal feature encoding is called **one-hot encoding**. Let's take feature `Lot Shape` as example:"]},{"cell_type":"code","metadata":{"id":"umnN487SYWwt"},"source":["X_nominal[\"Lot Shape\"].value_counts(dropna=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBDNXPV8Y6MK"},"source":["With one-hot encoding, feature `Lot Shape` gets replaced by binary features, one for each of the possible values presented by `Lot Shape`. Let's start importing the scikit-learn resources we're gonna need:"]},{"cell_type":"code","metadata":{"id":"k_IXgN2jfavO","executionInfo":{"status":"ok","timestamp":1601407140047,"user_tz":180,"elapsed":967,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import make_column_transformer"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejmOiI9dfc4d"},"source":["The `make_column_transformer` from the `compose` module allows us to specify encodings for specific features.\n","\n","- Let's understand the code below:\n","```python \n","nominal_encoder = make_column_transformer(\n","                                            (OneHotEncoder(sparse=False), [\"Lot Shape\"]),\n","                                            remainder=\"drop\")\n","```\n","> The `sparse=False` attribute indicates that the output should be a regular numpy array, rather than a more efficient scipy sparse matrix. Even if the former is less efficient, it is more readable and helps to keep the notebook understandable.\n","\n","- Creates the column transformer, specifying that a `OneHotEncoder()` should be applied to `[\"Lot Shape\"]`, and that the remainder of the features should be dropped. \n","```python \n","encoded_data = nominal_encoder.fit_transform(X_nominal)\n","```\n","- Applies the `fit_transform()` method to `X_nominal`. \n","```python\n","encoded_data = pd.DataFrame(encoded_data,\n","                              columns=nominal_encoder.get_feature_names())\n","```\n","- Wrap the numpy array as a DataFrame, retrieving the feature names created by the encoder.\n","\n","Let's see the output of that:"]},{"cell_type":"code","metadata":{"id":"SRdSZWq0Xm5L"},"source":["nominal_encoder = make_column_transformer(\n","                                          (OneHotEncoder(sparse=False), [\"Lot Shape\"]),\n","                                          remainder=\"drop\")\n","encoded_data = nominal_encoder.fit_transform(X_nominal)\n","encoded_data = pd.DataFrame(encoded_data,\n","                            columns=nominal_encoder.get_feature_names())\n","encoded_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqdEXvOhgjiY"},"source":["Note that the features are now labeled as a function of the feature values. For each sample, only one of the enconded feature values equals 1.0, since that was the original value before the encoding.\n","\n","> You can imagine that this type of encoding explodes the number of features in a dataframe, right? Let's see by how much!"]},{"cell_type":"code","metadata":{"id":"K5npC30hbHVo"},"source":["nominal_pipe = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(sparse=False))\n","pd.DataFrame(nominal_pipe.fit_transform(X_nominal),\n","             columns=nominal_pipe.steps[1][1].get_feature_names(nominal))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ikmdfu2AWnt"},"source":["> Note that we could have used better strategies than a `SimpleImputer(strategy='most_frequent')`, but that would make the notebook even longer üôÉ"]},{"cell_type":"markdown","metadata":{"id":"uPZddryoUVOa"},"source":["## Preparing numerical features"]},{"cell_type":"markdown","metadata":{"id":"n9RDfbKyAy_m"},"source":["When dealing with numerical features, our major concerns regard the data scale and distribution. Scikit-learn offers several resources to address these issues, as we'll see below."]},{"cell_type":"markdown","metadata":{"id":"bpwr8XhT2aQS"},"source":["### Feature scaling"]},{"cell_type":"markdown","metadata":{"id":"VFXuhkEv2bhd"},"source":["A very important aspect in data preprocessing concerns feature scaling. Take an algorithm that considers Euclidean distances, such as kNN. If a feature has much larger values than another, the relative importance of these features will be unbalanced. Sometimes, this comes in your favor, since the features with larger values are indeed the most important. Other times, your model will be undervaluing the most relevant features.\n","\n","Several scaling techniques can be used to make different features present the same scale, but they only make sense when we're talking about numerical features. For now, let's isolate the numerical features from the remaining. "]},{"cell_type":"code","metadata":{"id":"g9pQPaBJBctM"},"source":["X_num = X[numerical]\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2auOD55lw2S"},"source":["X_num.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0fGAaJ8BpaW"},"source":["Note that now we're working with 35 features. Let's move on to the techniques so we can understand our options."]},{"cell_type":"markdown","metadata":{"id":"Ofvk01QnB0VH"},"source":["#### Fitting to a range"]},{"cell_type":"markdown","metadata":{"id":"rxVZkpHmCMfu"},"source":["The first thing we notice about the numerical features of this dataset is that feature ranges are very different. Let's check, for instance, the range differences between features `Overall Qual`, `Year Built`, and `Gr Liv Area`:"]},{"cell_type":"code","metadata":{"id":"XiDiITnqDaIz"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mr9NVhU_GLS8"},"source":["We can scale all features to the `[0,1]` range using the `MinMaxScaler` preprocessor:"]},{"cell_type":"code","metadata":{"id":"IavcBRmeBopN"},"source":["from sklearn.preprocessing import MinMaxScaler\n","minmax = MinMaxScaler()\n","X_num = pd.DataFrame(minmax.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3MJViDbMTsW"},"source":["Note that now the min and max values for each feature now match:"]},{"cell_type":"code","metadata":{"id":"4NXcY-KfMNO-"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYKk_VaYJXAl"},"source":["> Note how the range scaling made the means of `Year Built` and `Gr Liv Area` different, when they were originally very close!"]},{"cell_type":"markdown","metadata":{"id":"nb9ZeONSB5Aj"},"source":["#### Centering the data"]},{"cell_type":"markdown","metadata":{"id":"1gRjZ1_VM7ev"},"source":["Another transformation commonly applied to numerical data is centering the data so it presents zero mean. We can do this transformation with the `StandardScaler` preprocessor, following the same pattern adopted above:"]},{"cell_type":"code","metadata":{"id":"XpprbR_zCBBf"},"source":["from sklearn.preprocessing import StandardScaler\n","zero_mean = StandardScaler(with_std=False)\n","X_num = pd.DataFrame(zero_mean.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJvwl9SfNduT"},"source":["> The parameter `with_std` defines whether `StandardScaler` also scales the variance. Here we disabled it not to spoil the next topic üòÖ"]},{"cell_type":"markdown","metadata":{"id":"waxf42qbJj9g"},"source":["After the transformation, all features present zero mean:"]},{"cell_type":"code","metadata":{"id":"gISyFRyCN8IG"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iPmord4N92w"},"source":["> Computers have a big trouble matching exact numbers when dealing with real values. The values shown in the mean column above are small enough to be considered zero by most algorithms."]},{"cell_type":"markdown","metadata":{"id":"6SJ9zRcPCExv"},"source":["#### Scaling the variance"]},{"cell_type":"markdown","metadata":{"id":"T_dLd0GJOJl3"},"source":["A final transformation that is often applied to numerical data is scaling the variance of the data to one. \n","\n","> It is so common that the `StandardScaler` preprocessor does it by default! Since we're doing it step-wisely, we will tell the preprocessor it doesn't need to center the data again (`with_mean=False`)."]},{"cell_type":"code","metadata":{"id":"fPO-a2_zCEYl"},"source":["unit_variance = StandardScaler(with_mean=False)\n","X_num = pd.DataFrame(unit_variance.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pihgmfwvOzLy"},"source":["After the transformation, all features present unit variance:\n","\n","> And, by definition, unit standard deviation!"]},{"cell_type":"code","metadata":{"id":"nBzwDzogOto9"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].var()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMcQSs-FOyME"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CZYqfkgPDZ-"},"source":["#### Using pipelines"]},{"cell_type":"markdown","metadata":{"id":"j4HPb7FqPHSf"},"source":["As discussed in the first steps with scikit-learn, pipelines are the best way to combine all the steps in a machine learning experiment. Here, we're gonna use them to encapsulate all the transformations applied to numerical features:"]},{"cell_type":"code","metadata":{"id":"OsiVAqdBCuEu"},"source":["from sklearn.impute import KNNImputer\n","num_pipe = make_pipeline(KNNImputer(), MinMaxScaler(), StandardScaler())\n","\n","X_num = X[numerical]\n","X_num = pd.DataFrame(num_pipe.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdpeZ-lbP48u"},"source":["X_num[[\"Overall Qual\", \"Year Built\", \"Gr Liv Area\"]].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKKksMYKURex"},"source":["### Transforming distributions"]},{"cell_type":"markdown","metadata":{"id":"L4mcXeY0UbCc"},"source":["The scaling conducted above does not affect the distribution shape of the data. From many aspects, having the data distributed according to a normal distribution is desirable. \n","\n","> In pandas-zero, we discussed how to transform a logarithmic distribution into a normal distribution.\n","\n","> We also discussed how to check if a distribution differs from a normal distribution. Tip: the second value in each parenthesis should be greater than or equal to 0.05."]},{"cell_type":"code","metadata":{"id":"gJRuD1padUbV"},"source":["from scipy.stats import normaltest\n","X_num.apply(normaltest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SlpvZQZbGvLT"},"source":["Scikit-learn offers two preprocessing components to transform distributions into normal distributions, which we discuss next. "]},{"cell_type":"markdown","metadata":{"id":"-uKWWuPAGNhu"},"source":["#### Quantization"]},{"cell_type":"markdown","metadata":{"id":"yJd9A2OKG2VL"},"source":["Quantization is the act of replacing a feature value for the quantile it belongs to. This is a non-parametric transformation that can be mapped to different distributions. scikit-learn provides the `QuantileTransformer` preprocessor, where we can configure the desired output distribution using the `output_distribution` argument:"]},{"cell_type":"code","metadata":{"id":"rElod5dWHAvw"},"source":["from sklearn.preprocessing import QuantileTransformer\n","num_pipe = make_pipeline(KNNImputer(),\n","                         MinMaxScaler(),\n","                         StandardScaler(),\n","                         QuantileTransformer(output_distribution='normal'))\n","\n","X_num = X[numerical]\n","X_num = pd.DataFrame(num_pipe.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lxj2Oq_rHpSf"},"source":["X_num.apply(normaltest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uEu4oibKANX"},"source":["From the results above, we see that only the `Yr Sold` feature is now matched with a normal distribution.\n","\n","> Note, however, that the p-value (second value in each parenthesis) increases for most features, indicating that the transformed distributions are closer to a normal distribution than the original ones."]},{"cell_type":"markdown","metadata":{"id":"zUOjFY5dKhHF"},"source":["#### Power transformations"]},{"cell_type":"markdown","metadata":{"id":"LNRcSlTZKjWG"},"source":["These parametric transformations are similar in spirit to the logarithmic transformation we discussed in pandas-zero. scikit-learn provides two options within the `PowerTransformer` preprocessor:\n","\n","* the Yeo-Johnson transform, which works for positive and negative values;\n","* the Box-Cox transform, which only works for strictly positive values.\n","\n","Since we have centered the data, we have to use the Yeo-Johnson transform (the default method for `PowerTransform`):"]},{"cell_type":"code","metadata":{"id":"vHYucFA2H5lw"},"source":["from sklearn.preprocessing import PowerTransformer\n","num_pipe = make_pipeline(KNNImputer(), \n","                         MinMaxScaler(),\n","                         StandardScaler(),\n","                         PowerTransformer())\n","\n","X_num = X[numerical]\n","X_num = pd.DataFrame(num_pipe.fit_transform(X_num), columns=X_num.columns)\n","X_num.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lH1vU9BSH5m_"},"source":["X_num.apply(normaltest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vlU1UWjLuYM"},"source":["This time, `Yr Sold` does not match a normal distribution. Yet, a larger number of features do, namely `Overall Qual`, `1st Flr SF`, `Gr Liv Area`, and `TotRms AbvGrd`. "]},{"cell_type":"markdown","metadata":{"id":"r2kSddcvLSng"},"source":["## Wrapping it all"]},{"cell_type":"markdown","metadata":{"id":"YC2euao1ltqU"},"source":["Although we should always deal with features according to their type, we can prepare our data without having to split our dataset as we did above:"]},{"cell_type":"code","metadata":{"id":"HJmUiWp3XoMX","executionInfo":{"status":"ok","timestamp":1601409029390,"user_tz":180,"elapsed":792,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["full_pipe = make_column_transformer(\n","                                    (ordinal_pipe, ordinal),\n","                                    (nominal_pipe, nominal),\n","                                    (num_pipe, numerical)\n","                                    )"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"97GTly_CnpEq"},"source":["In order to access feature names produced by the process of transforming nominal features, we'll fit that pipeline to the data, even if we're not gonna use it to transform the data:"]},{"cell_type":"code","metadata":{"id":"QlowliT7dCoJ","executionInfo":{"status":"ok","timestamp":1601409218125,"user_tz":180,"elapsed":1090,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["nominal_pipe.fit(X[nominal])\n","nominal_encoder = nominal_pipe.steps[1][1] # this accesses the OneHotEncoder within the pipeline\n","nominal_transformed = nominal_encoder.get_feature_names(nominal).tolist()"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_A9XRQMHoNfI"},"source":["The feature names for the whole dataset will be the concatenation of the following features:"]},{"cell_type":"code","metadata":{"id":"eEnPqjF0ZJS1","executionInfo":{"status":"ok","timestamp":1601409232147,"user_tz":180,"elapsed":960,"user":{"displayName":"Patr√≠cia Sayonara","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjkd_tWSbjcDpYlIa1tZVWavtxcPZ-gsrvwG2d0_A=s64","userId":"01856322366072644782"}}},"source":["column_names = ordinal + nominal_transformed + numerical"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ek1OK1EroS7Q"},"source":["We can now transform the data, preserving the dataframe feature names and therefore its readibility:"]},{"cell_type":"code","metadata":{"id":"3WWuDvPVYn6g"},"source":["X_transformed = pd.DataFrame(full_pipe.fit_transform(X),\n","                             columns=column_names)\n","X_transformed.head()"],"execution_count":null,"outputs":[]}]}